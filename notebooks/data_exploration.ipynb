{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47eccb9f",
   "metadata": {},
   "source": [
    "# Preprocessing & Data Exploration  \n",
    "**Notebook:** HDF daily laoder & dataset structure checks \n",
    "\n",
    "**Project Path:** `/Users/laval/spotify`\n",
    "\n",
    "This notebook provides a first exploration of the **daily HDF data dumps** used \n",
    "\n",
    "## Dataset Structure (per day)\n",
    "\n",
    "Each daily folder contains ≈ **40 HDF files**, which materialize into **4 core dataframes**:\n",
    "\n",
    "1. **Playlist–Track info**  \n",
    "   - Playlist ID  \n",
    "   - Track ID  \n",
    "   - Track name  \n",
    "   - Artist ID  \n",
    "   - Album ID  \n",
    "   - **Track popularity that day**\n",
    "   - File name: \"{date}_playlist_track_info_{i}.hdf\"\n",
    "\n",
    "2. **Playlist → Track list**  \n",
    "   Complete track list for every playlist observed that day.\n",
    "   - File name: \"{date}_playlist_ids_with_track_ids_{i}.hdf\"\n",
    "\n",
    "3. **Playlist metadata**  \n",
    "   - Playlist ID  \n",
    "   - Owner (Spotify / FILTER/Sony / Dexter/Universal / Toxify/Warner / …)  \n",
    "   - Description  \n",
    "   - **Follower count per day**  \n",
    "   - Track count \n",
    "   - File name: \"{date}_playlists_with_features_{i}.hdf\" \n",
    "\n",
    "4. **Track IDs observed that day**  \n",
    "   ~1M tracks/day, collected across playlists.\n",
    "   - File name: \"{date}_track_ids_{i}.hdf\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a822ccbc",
   "metadata": {},
   "source": [
    "## HDF loader \n",
    "\n",
    "Adapted from @zeijena's script\n",
    "\n",
    "- Listing the HDF files for selected dates\n",
    "- Loading each of the 4 expected dataframes\n",
    "- Printing shapes & sample rows\n",
    "- Validating consistent schema across days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b7a4809",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "398d85b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Folder: 2019-01-09\n",
      "   Parsed date: 2019-01-09\n",
      "[playlist_track_info] part 1\n",
      "Error reading 2019-01-09_playlist_track_info_1.hdf: Missing optional dependency 'pytables'.  Use pip or conda to install pytables.\n",
      "[playlist_ids_with_track_ids] part 1\n",
      "Error reading 2019-01-09_playlist_ids_with_track_ids_1.hdf: Missing optional dependency 'pytables'.  Use pip or conda to install pytables.\n",
      "[playlists_with_features] part 1\n",
      "Error reading 2019-01-09_playlists_with_features_1.hdf: Missing optional dependency 'pytables'.  Use pip or conda to install pytables.\n",
      "[track_ids] part 1\n",
      "Error reading 2019-01-09_track_ids_1.hdf: Missing optional dependency 'pytables'.  Use pip or conda to install pytables.\n"
     ]
    }
   ],
   "source": [
    "# --- PATHS ---\n",
    "main_folder_path = '/Users/laval/spotify/data'  # daily folders\n",
    "\n",
    "# --- PARAMS ---\n",
    "N_PARTS = 10  # set to 10 on full daily data \n",
    "\n",
    "# List date folders\n",
    "all_folders = sorted(\n",
    "    [folder for folder in os.listdir(main_folder_path)\n",
    "     if os.path.isdir(os.path.join(main_folder_path, folder))]\n",
    ")\n",
    "\n",
    "# first day \n",
    "all_folders = all_folders[:1]\n",
    "\n",
    "for idx, folder_name in enumerate(all_folders):\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"Folder: {folder_name}\")\n",
    "    folder_path = os.path.join(main_folder_path, folder_name)\n",
    "\n",
    "    if not os.path.exists(folder_path):\n",
    "        print(\"Folder does not exist, skip\")\n",
    "        continue\n",
    "\n",
    "    # parse date with folder_name format 'YYYY-MM-DD'\n",
    "    try:\n",
    "        date_observed = datetime.strptime(folder_name, '%Y-%m-%d').date()\n",
    "        print(f\"   Parsed date: {date_observed}\")\n",
    "    except ValueError:\n",
    "        date_observed = None\n",
    "        print(\"Could not parse folder name as date\")\n",
    "\n",
    "    # =========================================\n",
    "    # 1) playlist_track_info\n",
    "    # =========================================\n",
    "    dfs = []\n",
    "    for i in range(1, N_PARTS + 1):\n",
    "        print(f\"[playlist_track_info] part {i}\")\n",
    "        file_name = f\"{folder_name}_playlist_track_info_{i}.hdf\"\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "\n",
    "        if not os.path.exists(file_path):\n",
    "            print(f\"   - file {file_name} not found, skip\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            df = pd.read_hdf(file_path, key='/playlist_track_info')\n",
    "            df['track.popularity'] = pd.to_numeric(df['track.popularity'], errors='coerce')\n",
    "            dfs.append(df)\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {file_name}: {e}\")\n",
    "            break # Skip the entire day if there is an error with any file \n",
    "\n",
    "    big_df_playlist_track_info = pd.concat(dfs, ignore_index=True) if dfs else None\n",
    "    if big_df_playlist_track_info is not None:\n",
    "        print(\"1) playlist_track_info loaded:\", big_df_playlist_track_info.shape)\n",
    "\n",
    "    # =========================================\n",
    "    # 2) playlist_ids_with_track_ids\n",
    "    # =========================================\n",
    "    dfs = []\n",
    "    for i in range(1, N_PARTS + 1):\n",
    "        print(f\"[playlist_ids_with_track_ids] part {i}\")\n",
    "        file_name = f\"{folder_name}_playlist_ids_with_track_ids_{i}.hdf\"\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "\n",
    "        if not os.path.exists(file_path):\n",
    "            print(f\"   - file {file_name} not found, skip\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            df = pd.read_hdf(file_path, key='/playlist_ids_with_track_ids')\n",
    "            dfs.append(df)\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {file_name}: {e}\")\n",
    "            break\n",
    "\n",
    "    big_df_playlist_ids_with_track_ids = pd.concat(dfs, ignore_index=True) if dfs else None\n",
    "    if big_df_playlist_ids_with_track_ids is not None:\n",
    "        print(\"2) playlist_ids_with_track_ids loaded:\",\n",
    "              big_df_playlist_ids_with_track_ids.shape)\n",
    "\n",
    "    # =========================================\n",
    "    # 3) playlists_with_features\n",
    "    # =========================================\n",
    "    dfs = []\n",
    "    for i in range(1, N_PARTS + 1):\n",
    "        print(f\"[playlists_with_features] part {i}\")\n",
    "        file_name = f\"{folder_name}_playlists_with_features_{i}.hdf\"\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "\n",
    "        if not os.path.exists(file_path):\n",
    "            print(f\"   - file {file_name} not found, skip\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            df = pd.read_hdf(file_path, key='/playlists')\n",
    "            dfs.append(df)\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {file_name}: {e}\")\n",
    "            break\n",
    "\n",
    "    big_df_playlists = pd.concat(dfs, ignore_index=True) if dfs else None\n",
    "    if big_df_playlists is not None:\n",
    "        print(\"3) playlists_with_features loaded:\", big_df_playlists.shape)\n",
    "\n",
    "    # =========================================\n",
    "    # 4) track_ids\n",
    "    # =========================================\n",
    "    dfs = []\n",
    "    for i in range(1, N_PARTS + 1):\n",
    "        print(f\"[track_ids] part {i}\")\n",
    "        file_name = f\"{folder_name}_track_ids_{i}.hdf\"\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "\n",
    "        if not os.path.exists(file_path):\n",
    "            print(f\"   - file {file_name} not found, skip\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            df = pd.read_hdf(file_path, key='/gathered_track_ids')\n",
    "            dfs.append(df)\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {file_name}: {e}\")\n",
    "            break\n",
    "\n",
    "    big_df_track_ids = pd.concat(dfs, ignore_index=True) if dfs else None\n",
    "    if big_df_track_ids is not None:\n",
    "        print(\"4) track_ids loaded:\", big_df_track_ids.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc3016cc",
   "metadata": {},
   "source": [
    "## Inspecting daily HDF data (structural overview & quick sanity checks)\n",
    "\n",
    "Series of simple diagnostics to confirm that:\n",
    "- Each of the 4 expected daily dataframes was correctly loaded\n",
    "- The schema and column names match expectations\n",
    "- The number of rows per part/day is reasonable\n",
    "- Key columns such as `track.id`, `playlist.id`, `track.popularity` are present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99a35f03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "DAILY DATA STRUCTURE OVERVIEW\n",
      "====================================================================================================\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "*** PLAYLIST_TRACK_INFO\n",
      "Not loaded missing or failed HDF files\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "*** PLAYLIST_IDS_WITH_TRACK_IDS\n",
      "Not loaded missing or failed HDF files\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "*** PLAYLISTS\n",
      "Not loaded missing or failed HDF files\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "*** TRACK_IDS\n",
      "Not loaded missing or failed HDF files\n"
     ]
    }
   ],
   "source": [
    "# List of loaded daily dataframes (the loader script must have run first)\n",
    "loaded_dfs = {\n",
    "    \"playlist_track_info\": big_df_playlist_track_info,\n",
    "    \"playlist_ids_with_track_ids\": big_df_playlist_ids_with_track_ids,\n",
    "    \"playlists\": big_df_playlists,\n",
    "    \"track_ids\": big_df_track_ids,\n",
    "}\n",
    "\n",
    "print(\"=\"*100)\n",
    "print(\"DAILY DATA STRUCTURE OVERVIEW\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "for name, df in loaded_dfs.items():\n",
    "    print(\"\\n\" + \"-\"*100)\n",
    "    print(f\"*** {name.upper()}\")\n",
    "    \n",
    "    if df is None:\n",
    "        print(\"Not loaded missing or failed HDF files\")\n",
    "        continue\n",
    "\n",
    "    # Basic metadata\n",
    "    print(f\"   Shape: {df.shape[0]:,} rows × {df.shape[1]} columns\")\n",
    "    print(\"   Memory usage: {:.2f} MB\".format(df.memory_usage(deep=True).sum() / 1e6))\n",
    "\n",
    "    # Columns\n",
    "    print(\"   Columns:\", list(df.columns)[:15], \n",
    "          (\"...\" if len(df.columns) > 15 else \"\"))\n",
    "\n",
    "    # Non-null summary\n",
    "    print(\"\\n   Non-null counts:\")\n",
    "    display(df.notnull().sum().sort_values(ascending=False).head(10))\n",
    "\n",
    "    # Head\n",
    "    print(\"\\n   Preview:\")\n",
    "    display(df.head())\n",
    "\n",
    "    # Simple identifier check\n",
    "    id_candidates = [c for c in df.columns if \"id\" in c.lower()]\n",
    "    if id_candidates:\n",
    "        col = id_candidates[0]\n",
    "        print(f\"   Unique values in '{col}': {df[col].nunique():,}\")\n",
    "    else:\n",
    "        print(\"   (No obvious ID column found.)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d49fae68",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e7c88d7f",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
